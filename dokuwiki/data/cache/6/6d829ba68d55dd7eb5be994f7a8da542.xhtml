
<p>
Pour faire des essais, créons qqs fakedisk
</p>

<p>
le plus simple est de faire des “disques” de 1G
</p>

<p>
la commande
</p>
<pre class="code"> dd if=/dev/zero of=fakedisk_a bs=1G count=1</pre>

<p>
fera l&#039;affaire mais sera lente
</p>

<p>
tandis que:
</p>
<pre class="code"> dd if=/dev/zero of=faka count=1 seek=2M</pre>

<p>
sera quasi instantanée, elle crée un fichier de 1G après avoir “avancé” instantanément de 2 millions (seek=2M)
de bloc de 512 octets.
</p>

<p>
faisons trois fakedisks de 1G, et trois fakedisks de 1.5 Gigas
</p>
<pre class="code">dd if=/dev/zero of=faka count=1 seek=2M
dd if=/dev/zero of=fakb count=1 seek=2M
dd if=/dev/zero of=fakc count=1 seek=2M   

dd if=/dev/zero of=fakd count=1 seek=3M
dd if=/dev/zero of=fake count=1 seek=3M
dd if=/dev/zero of=fakf count=1 seek=3M</pre>
<pre class="code">big data # ls -lh fak?
-rw-r--r-- 1 root root 1.1G Apr 26 15:59 faka
-rw-r--r-- 1 root root 1.1G Apr 26 16:00 fakb
-rw-r--r-- 1 root root 1.1G Apr 26 16:01 fakc
-rw-r--r-- 1 root root 1.6G Apr 26 16:06 fakd
-rw-r--r-- 1 root root 1.6G Apr 26 16:06 fake
-rw-r--r-- 1 root root 1.6G Apr 26 16:06 fakf
big data # </pre>

<p>
L&#039;idée est de faire comme si nous disposions de 6 disques
qu&#039;il faut faire apparaître comme des périphériques, pour cela nous créons 6 “loop”
avec les commandes
</p>
<pre class="code">big data # losetup -f faka
big data # losetup -f fakb
big data # losetup -f fakc
big data # losetup -f fakd
big data # losetup -f fake
big data # losetup -f fakf
big data # losetup -la
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop0         0      0         1  1 /boot/srcd.iso
/dev/loop1         0      0         0  0 /data/faka
/dev/loop2         0      0         0  0 /data/fakb
/dev/loop3         0      0         0  0 /data/fakc
/dev/loop4         0      0         0  0 /data/fakd
/dev/loop5         0      0         0  0 /data/fake
/dev/loop6         0      0         0  0 /data/fakf
big data # </pre>

<p>
On constate que les 6 loops ont été créées, et correspondent aux pseudo-disques /dev/loop1,2,..,6
</p>

<p>
(Il y avait dans l&#039;exemple ci-dessus déjà un pseudo disk loop0 )
</p>

<p>
Nous allons créer un disque raid5 avec les 3 premiers disques de 1Giga
</p>
<pre class="code">le pseudo disque raid assemblé s&#039;appellera md100
le niveau de raid (level) sera égal à 5 (-l 5)
il sera constitué de 3 disques (-n 3)
ces disques sont /dev/loop1, /dev/loop2, /dev/loop3

!! attention il est très probable que ces chiffres soient différent dans votre situation !!</pre>

<p>
Il est fortement recommandé, dans un autre xterm de lancer la commande
</p>
<pre class="code">watch cat /proc/mdstat</pre>

<p>
Afin d&#039;avoir une vision dynamique de l&#039;état de vos disques raid.
</p>
<pre class="code">mdadm --create md100 -l 5 -n 3  /dev/loop2 /dev/loop1 /dev/loop3</pre>

<p>
vous verrez le périphérique md100 se créer et se “synchroniser”
en l&#039;occurrence  c&#039;est inutile vu qu&#039;il s&#039;agit d&#039;un nouveau raid, il est inutile de le vérifier, nous aurions pu ajouter à la commande de création l&#039;option “–assume-clean” pour accélérer le processus
</p>
<pre class="code">__A Faire__
 regarder le contenu de /dev/md/
 fdisk -l /dev/md100
 </pre>

<p>
Nous avons bien un disque de 2Gigas !
</p>

<p>
ce disque peut être partitionné mais les partitions ne seront pas numérotées comme pour des disques durs physiques réels (sda1, sda2) elles seraient numérotées md100p1, md100p2 etc
</p>

<p>
Pour la simplicité nous utiliserons l&#039;entièreté du disque md100
il suffit de le formater pour pouvoir le monter
</p>
<pre class="code">mkfs.xfs /dev/md/md100  
mount /dev/md100 /mnt/__quelquepart__  (ce que vous voulez comme point de montage)</pre>

<p>
simulons maintenant une panne d&#039;un des disques de 1Giga
</p>
<pre class="code">mdadm --manage /dev/md/md100 --fail /dev/loop1</pre>

<p>
le système tient… il ne manque qu&#039;un disque.. 
constatez ce qui apparait dans la fenêtre d&#039;état du raid.
</p>

<p>
Ajoutons mainteant un nouveau disque au raid mais prenos cette fois ci un disque
de 1.5 Giga, d&#039;abord enlevons le disque “mort”
</p>
<pre class="code">mdadm --manage /dev/md/md100 --rem /dev/loop1</pre>

<p>
puis ajoutons le disque4
</p>
<pre class="code">mdadm --manage /dev/md/md100 --add /dev/loop4</pre>

<p>
Observez la “reconstruction” du disque md100
</p>

<p>
Ajoutons les 2 autres disque de 1.5 giga au raid. Ils ne seront aps utilisé mais seront là prêt au cas où une panne survient.
</p>
<pre class="code">mdadm --manage /dev/md/md100 --add /dev/loop5
mdadm --manage /dev/md/md100 --add /dev/loop6</pre>

<p>
Constatez le mode “Spare” (secours) pour ces 2 nouveaux disques dans la fenêtre d&#039;observation
</p>

<p>
Déclenchons une nouvelle panne, du disque 2 (1Giga)
</p>
<pre class="code">mdadm --manage /dev/md/md100 --fail /dev/loop2</pre>

<p>
Le système déclenche automatiquement la reconstruction du raid en utilisant un des disques “spare”
</p>

<p>
!! il est important d&#039;attendre que la reconstruction soit achevée avant de déclencher la dernière panne du dernier disque de 1Giga !!
</p>
<pre class="code">mdadm --manage /dev/md/md100 --fail /dev/loop3</pre>

<p>
Nous pouvons maintenant retirer les loop2 et 3 du raid
</p>
<pre class="code">big ~ # mdadm --manage /dev/md/md100 --rem /dev/loop2
mdadm: hot removed /dev/loop2 from /dev/md/md100
big ~ # mdadm --manage /dev/md/md100 --rem /dev/loop3
mdadm: hot removed /dev/loop3 from /dev/md/md100
big ~ # </pre>

<p>
Les trois disques du raid ont été remplacés par des disques plus grands…
En théorie notre disque devrait pouvoir avoir une capacité de 3Gigas, mais il en a toujours que 2.
</p>

<p>
Il a été initialisé pour une taille de 2Gigas, les disques plus grands que nous avons incorporés ne sont pas exploités au maximum, c&#039;est normal ils étaient là en guise de secours.
</p>

<p>
L&#039;important est que le système a conservé le disque md100 monté, les données qu&#039;il contient ont toujours été accessibles.
</p>

<p>
Il existe un moyen d&#039;étendre le “container raid” pour profiter du fait que les disques sous-jacents sont maintenant plus grand (disques 4,5,6) que les disques d&#039;origine (1,2,3).
</p>
<pre class="code">big ~ # mdadm --grow /dev/md/md100 --size=max
mdadm: component size of /dev/md/md100 has been set to 1571840K
unfreeze
big ~ #</pre>

<p>
Cependant le file system XFS a lui était fait sur un disque de 2Giga, et ne contiendra donc pas plus.
</p>

<p>
Il existe aussi un moyen d&#039;agrandir un filesystem XFS pour qu&#039;il prenne l&#039;espace disponible à sa suite, c&#039;est la commande :
</p>
<pre class="code">xfs_growfs /dev/md/md100</pre>

<p>
Après quoi, quand le xfs_grow est complété, regardez le résultat de df -h
</p>

<p>
vous y verrez un filesystem de 3giga
</p>

<p>
Le tout sans avoir jamais arrêté la machine !
</p>

<p>
Ceci était pour le XFS, qui ne peut qu&#039;être agrandi ! jamais réduit.
<a href="/dokuwiki/doku.php?id=instruction_pour_l_ext3" class="wikilink2" title="instruction_pour_l_ext3" rel="nofollow">instruction pour l&#039;ext3</a> <a href="/dokuwiki/doku.php?id=instruction_pour_l_ext4" class="wikilink2" title="instruction_pour_l_ext4" rel="nofollow">instruction pour l&#039;ext4</a> <a href="/dokuwiki/doku.php?id=instruction_pour_reiserfs" class="wikilink2" title="instruction_pour_reiserfs" rel="nofollow">instruction pour reiserfs</a>
</p>

<p>
pour stopper le raid
</p>
<pre class="code">umount le disque raid
mdadm --stop /dev/md/md100</pre>

<p>
détacher les loop
</p>
<pre class="code">losetup -d /dev/loop1 etc etc</pre>

<p>
<a href="/dokuwiki/doku.php?id=instruction_pour_ajouter_un_disque_au_raid" class="wikilink1" title="instruction_pour_ajouter_un_disque_au_raid">instruction pour ajouter un disque au raid</a>
</p>
